On [[the-predictive-mind]]

# ---

A model I wrote out, mostly to get GPTs take on it: https://chatgpt.com/c/67067489-18a4-800c-8058-f03edd26679e

I'm reading the predictive mind right now and I'm trying to grok the model. I'm going to try to describe certain aspects of it in my own words. This kinda makes sense to me, but I know it's wrong in certain ways, I just don't know how.

I'll start with a distinction I have in my conception that I'm not sure if made in predictive coding. So there's the sensory inputs which are the raw data coming through eyes, ears, etc. Then separately there's the "3d multimedia object" (my own term) which is the thing which my phenomenological experience *is* i.e. a 3d world of objects, colours, sounds, feelings, etc. The conjunction of all of these I'm referring to as a single 3d multimedia object which is generated by the mind (in my conception, that thing *is not* just the raw input from the senses)

In my conception I see 2 separate predictive systems in the mind: 
- system A which takes in a sensory input (and prev inputs, priors, etc) and outputs a 3d multimedia object
- system B which given the past multimedia object, sensory inputs, priors, etc can produce a predicted next multimedia object
- you can run system A in reverse to produce expected sensory inputs, given a multimedia object (which can be produced for future times using system B)

What the mind is doing with these:
- The mind produces the expected next state of the world as a multimedia object
- It passes that backwards through system A to produce expected sensory inputs which would produce that state
- Then we receive the real sensory inputs. The difference between the expected and real back propagate through system A, tweaking the parameters such that next time their prediction will be closer to the real sensory inputs. 
- The real sensory inputs flow through system A to produce a multimedia object. The difference between that object and the predicted one also outputs prediction errors. These back-propagate through B to update it's parameters.

I don't see the distinction in the book between A and B. I think it's all the same. They seem different to me though.

# ---

The model I proposed where sensory inputs flow in and a 3d multimedia object flows out isn't quite right.

There is a hierarchy/layers in the system, but at each layer we're outputting predictions (same as NNs?). My experience isn't the final output from that system, it's the conjunction of _all_ predictions across all layers of the system.  I experience both the raw colours of the table and the concept of a table as a whole all in one unified experience.

top = high level concepts (person, chair, dog)
low = sensory inputs (colour, light, texture)

# ---

This example is worth reviewing again and groking:

TM: Can you give a concrete example? It can be a bit contrived/toyish that's fine. Again keep zoomed in on a single layer, one that's somewhere in the middle. What's are the hypotheses this layer has about the world? What is the higher level providing to it? What's the lower level that it's predicting?

GPT: Sure! Let's use a concrete example involving visual perception:

**Scenario:**

You're looking at a scene that includes a **cat** sitting on a mat.

**Focus on a Middle Layer:**

Let's zoom in on a middle layer in your visual processing hierarchy responsible for recognizing **features like curved edges and textures**.

**Components of This Layer:**

1. **Layer's Own Hypotheses:**
    
    - This layer hypothesizes about the presence of specific features such as **curved shapes, fur textures, and contours** that are common in animals like cats.
2. **Input from Higher Layer:**
    
    - The higher layer predicts that there's a **cat** in the scene.
    - It sends down expectations for features associated with cats (e.g., whiskers, pointy ears, tail).
3. **Predicting Lower-Level Inputs:**
    
    - Based on its own hypotheses and the higher-layer input, this middle layer predicts that the lower layer should detect **specific lines, edges, and textures** corresponding to cat features.
4. **Comparison with Lower-Level Outputs:**
    
    - The lower layer provides actual sensory data about basic visual elements like **simple edges and colors**.
    - The middle layer compares its predictions with these inputs.
5. **Prediction Error:**
    
    - If the expected features (e.g., curved whiskers) are not detected because the cat is partially hidden, there's a mismatch.
    - This generates a **prediction error**.
6. **Resulting Adjustments:**
    
    - The prediction error is sent upward to inform the higher layer that its hypothesis ("there's a full cat visible") may need adjustment.
    - The middle layer also updates its own hypotheses to account for the missing features.

**Summary:**

- **Layer's Hypotheses:** Recognition of specific features like curved edges and textures typical of a cat.
- **Higher-Level Input:** Expectation that a cat is present, providing context for expected features.
- **Lower-Level Prediction:** Anticipation of basic visual elements (lines, edges) forming the cat's features.
- **Prediction Error:** Occurs when there's a discrepancy between expected and actual lower-level inputs.

This example illustrates how a middle layer integrates higher-level predictions with its own hypotheses to predict lower-level inputs, and how prediction errors guide the updating of hypotheses across the hierarchy.
